---
title: 支持向量机（Support Vector Machine）
date: 2020-06-22 12:23:08
categories:
  - machine learning
tags:
  - SVM
  - machine learning
---
svm以及logistic回归都是基于感知机发展出来的，三者都是线性分类器（线性分类器通常是要学习出一个超平面，然后用它进行新数据的预测）。

感知机分类模型选超平面时，其实有很多种选择，但如何确定最佳超平面？我们认为数据点应该尽量远离超平面或决策边界，因为如果数据点距离决策边界很近，决策边界的一点点改动，都会造成一些数据点的预测类别改变，这是不稳定的。

例如，下图中的所有虚线做分类线都能将红色圆点和绿色加号完全分开，但直观上会觉得距离两类数据点距离都最远的蓝色虚线是最佳分类线选择，其对于新数据的预测分类的正确性要比其他虚线的高。

![](markdown-img-paste-20200622131000250.png)
总结一下，对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的可靠性也越大。因此我们需要让所选择的超平面能够最大化这个“间隔”值。那这个“间隔值”到底是什么？又如何去最大化？
先来看看间隔值如何定义。两个间隔定义：

1\.   函数间隔（functional margin）

在超平面 ```w*x + b = 0``` 确定的情况下，```| w*x + b |``` 能够表示点 **x** 距离超平面的远近（根据点到面距离公式知此为分子），而通过观察 ```w*x + b``` 的符号与标签 y 的是否一致可判断分类是否正确。即可以用 ```y*(w*x + b)``` 的正负来判定分类的正确性。

定义超平面关于训练集中某一点 (x<sup>(i)</sup>，y<sup>(i)</sup>) 的函数间隔

![](markdown-img-paste-20200622131105197.png)

则超平面关于训练集的函数间隔 ![](/SVM/markdown-img-paste-20200622141314674.png)，则为超平面关于训练集中所有点(x<sup>(i)</sup>，y<sup>(i)</sup>) 的函数间隔的最小值![](markdown-img-paste-20200622141424827.png)

![](markdown-img-paste-2020062214152566.png) （n为样本数）

上式也说明了，当衡量一个平面到数据集的“远近”时，其实只需要看样本点中距离最近的点。

假设现在要最大化函数间隔，你会发现当等比例放大 w，b 时，函数间隔的确变大了，成了原来的两倍

![](markdown-img-paste-20200622141807620.png)

但超平面没有变化（```2w*x + 2b = 0``` ⟺ ```w*x + b = 0```），即对数据分类（预测结果）没有任何改变。为了排除这种仅数值变化而非超平面优化的“变化”，需要换一种“间隔”定义。

2\.   几何间隔（geometrical margin）

假定对于一个点 x<sup>(i)</sup> ，令其垂直投影到超平面上的对应点为 x<sup>(i)</sup><sub>0</sub> ，w 是超平面的法向量，𝛄<sup>(i)</sup>为样本 x<sup>(i)</sup> 到超平面的距离，如下图（a）所示：

![](markdown-img-paste-20200622142211494.png)

根据图（b）向量关系 Ox = Ox<sub>0</sub> + x<sub>0</sub>x 可得 x 和 x<sub>0</sub> 关系

![](markdown-img-paste-20200622142334649.png)

已知x<sub>0</sub>是超平面上的点，并将上式两边同乘以 w<sup>T</sup>，得
![](markdown-img-paste-20200622142448162.png)

为了使“间隔”是个非负数，对𝛄乘上标签y，则得到超平面关于该点的几何间隔
![](markdown-img-paste-20200622142508140.png)
则超平面关于训练集的几何间隔，则为超平面关于训练集中所有点(x(i)，y(i))的几何间隔的最小值
