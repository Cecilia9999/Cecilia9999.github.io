<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="SVM,machine learning," />










<meta name="description" content="svm以及logistic回归都是基于感知机发展出来的，三者都是线性分类器（线性分类器通常是要学习出一个超平面，然后用它进行新数据的预测）。 感知机分类模型选超平面时，其实有很多种选择，但如何确定最佳超平面？我们认为数据点应该尽量远离超平面或决策边界，因为如果数据点距离决策边界很近，决策边界的一点点改动，都会造成一些数据点的预测类别改变，这是不稳定的。 例如，下图中的所有虚线做分类线都能将红色圆点">
<meta property="og:type" content="article">
<meta property="og:title" content="支持向量机（Support Vector Machine）">
<meta property="og:url" content="http://yoursite.com/2020/06/22/post/SVM/index.html">
<meta property="og:site_name" content="To be a developer">
<meta property="og:description" content="svm以及logistic回归都是基于感知机发展出来的，三者都是线性分类器（线性分类器通常是要学习出一个超平面，然后用它进行新数据的预测）。 感知机分类模型选超平面时，其实有很多种选择，但如何确定最佳超平面？我们认为数据点应该尽量远离超平面或决策边界，因为如果数据点距离决策边界很近，决策边界的一点点改动，都会造成一些数据点的预测类别改变，这是不稳定的。 例如，下图中的所有虚线做分类线都能将红色圆点">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622131000250.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622131105197.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622141314674.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622141424827.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-2020062214152566.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622141807620.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622142211494.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622142334649.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622142448162.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622142508140.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622151628643.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622151856916.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622151909599.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622152119434.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622152136578.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622152251359.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-2020062215234943.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622152516616.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622152731192.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622152845823.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622152908761.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622153012636.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-2020062215303654.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622153055518.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622153227572.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622153306933.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622153256418.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622153424253.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622153716976.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622153736254.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622153918993.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622154052110.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622154123722.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622154133324.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622154141279.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622154244273.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622154258718.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622154417179.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622155059313.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-2020062215513029.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622155139762.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-2020062215531661.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622155853126.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622160100659.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622160141953.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622160332289.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622160241913.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622160309786.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622160752822.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622160849906.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622160921418.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622160940732.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622161205332.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622161340954.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622161402249.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622161539510.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622161838872.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622161933825.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622162157612.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622162223233.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-2020062216282268.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622162848235.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622163347397.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622164414184.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622164654971.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622164721539.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622164837422.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622164851848.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622165106474.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622165210168.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622165241944.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-2020062216533710.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622170015932.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622170026716.png">
<meta property="og:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622170436384.png">
<meta property="article:published_time" content="2020-06-22T03:23:08.000Z">
<meta property="article:modified_time" content="2020-06-22T08:12:50.000Z">
<meta property="article:author" content="Cecilia Song">
<meta property="article:tag" content="SVM">
<meta property="article:tag" content="machine learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/06/22/post/SVM/markdown-img-paste-20200622131000250.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/06/22/post/SVM/"/>





  <title>支持向量机（Support Vector Machine） | To be a developer</title>
  








<meta name="generator" content="Hexo 4.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    <a href="https://github.com/Cecilia9999" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">To be a developer</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">A person who is worried about hair loss</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/22/post/SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Cecilia Song">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/selficon.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="To be a developer">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">支持向量机（Support Vector Machine）</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-22T12:23:08+09:00">
                2020-06-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/machine-learning/" itemprop="url" rel="index">
                    <span itemprop="name">machine learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>svm以及logistic回归都是基于感知机发展出来的，三者都是线性分类器（线性分类器通常是要学习出一个超平面，然后用它进行新数据的预测）。</p>
<p>感知机分类模型选超平面时，其实有很多种选择，但如何确定最佳超平面？我们认为数据点应该尽量远离超平面或决策边界，因为如果数据点距离决策边界很近，决策边界的一点点改动，都会造成一些数据点的预测类别改变，这是不稳定的。</p>
<p>例如，下图中的所有虚线做分类线都能将红色圆点和绿色加号完全分开，但直观上会觉得距离两类数据点距离都最远的蓝色虚线是最佳分类线选择，其对于新数据的预测分类的正确性要比其他虚线的高。</p>
<p><img src="markdown-img-paste-20200622131000250.png" alt=""><br>总结一下，对一个数据点进行分类，当超平面离数据点的“间隔”越大，分类的可靠性也越大。因此我们需要让所选择的超平面能够最大化这个“间隔”值。那这个“间隔值”到底是什么？又如何去最大化？先来看看间隔值如何定义。</p>
<h2 id="两个间隔定义"><a href="#两个间隔定义" class="headerlink" title="两个间隔定义"></a>两个间隔定义</h2><h3 id="1-函数间隔（functional-margin）"><a href="#1-函数间隔（functional-margin）" class="headerlink" title="1.   函数间隔（functional margin）"></a>1.   函数间隔（functional margin）</h3><p>在超平面 <code>w*x + b = 0</code> 确定的情况下，<code>| w*x + b |</code> 能够表示点 <strong>x</strong> 距离超平面的远近（根据点到面距离公式知此为分子），而通过观察 <code>w*x + b</code> 的符号与标签 y 的是否一致可判断分类是否正确。即可以用 <code>y*(w*x + b)</code> 的正负来判定分类的正确性。</p>
<p>定义超平面关于训练集中某一点 (x<sup>(i)</sup>，y<sup>(i)</sup>) 的函数间隔</p>
<p><img src="markdown-img-paste-20200622131105197.png" alt=""></p>
<p>超平面关于训练集的函数间隔 <img src="markdown-img-paste-20200622141314674.png" alt="">)，则为超平面关于训练集中所有点(x<sup>(i)</sup>，y<sup>(i)</sup>) 的函数间隔的最小值<img src="markdown-img-paste-20200622141424827.png" alt=""></p>
<p><img src="markdown-img-paste-2020062214152566.png" alt=""> （n为样本数）</p>
<p>上式也说明了，当衡量一个平面到数据集的“远近”时，其实只需要看样本点中距离最近的点。</p>
<p>假设现在要最大化函数间隔，你会发现当等比例放大 w，b 时，函数间隔的确变大了，成了原来的两倍</p>
<p><img src="markdown-img-paste-20200622141807620.png" alt=""></p>
<p>但超平面没有变化（<code>2w*x + 2b = 0</code> ⟺ <code>w*x + b = 0</code>），即对数据分类（预测结果）没有任何改变。为了排除这种仅数值变化而非超平面优化的“变化”，需要换一种“间隔”定义。</p>
<h3 id="2-几何间隔（geometrical-margin）"><a href="#2-几何间隔（geometrical-margin）" class="headerlink" title="2.   几何间隔（geometrical margin）"></a>2.   几何间隔（geometrical margin）</h3><p>假定对于一个点 x<sup>(i)</sup> ，令其垂直投影到超平面上的对应点为 x<sup>(i)</sup><sub>0</sub> ，w 是超平面的法向量，𝛄<sup>(i)</sup>为样本 x<sup>(i)</sup> 到超平面的距离，如下图（a）所示：</p>
<p><img src="markdown-img-paste-20200622142211494.png" alt=""></p>
<p>根据图（b）向量关系 Ox = Ox<sub>0</sub> + x<sub>0</sub>x 可得 x 和 x<sub>0</sub> 关系</p>
<p><img src="markdown-img-paste-20200622142334649.png" alt=""></p>
<p>已知x<sub>0</sub>是超平面上的点，并将上式两边同乘以 w<sup>T</sup>，得<br><img src="markdown-img-paste-20200622142448162.png" alt=""></p>
<p>为了使“间隔”是个非负数，对𝛄乘上标签y，则得到超平面关于该点的几何间隔</p>
<p><img src="markdown-img-paste-20200622142508140.png" alt=""></p>
<p>超平面关于训练集的几何间隔，则为超平面关于训练集中所有点(x<sup>(i)</sup>，y<sup>(i)</sup>) 的几何间隔的最小值</p>
<p><img src="markdown-img-paste-20200622151628643.png" alt=""></p>
<h3 id="为什么最大化“间隔”用的是几何而非函数间隔："><a href="#为什么最大化“间隔”用的是几何而非函数间隔：" class="headerlink" title="为什么最大化“间隔”用的是几何而非函数间隔："></a>为什么最大化“间隔”用的是几何而非函数间隔：</h3><p>同等程度缩放 w，b 时，函数间隔产生的变化仅为数值变化而非超平面的变化，而几何间隔不会变化，因为它只随超平面的变化而变化。<br>例如，将 w，b 放大2倍，函数间隔也被放大两倍（数值变化），但几何间隔不变（超平面不变，点到面距离也不变），所以当优化的是几何间隔时，这种超平面没变化的（数值）变化其实是无效、无变化的。<br><img src="markdown-img-paste-20200622151856916.png" alt=""></p>
<p><img src="markdown-img-paste-20200622151909599.png" alt=""></p>
<h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h4><p>1）数据正确分类时，该点的函数间隔就是 <code>| w*x + b |</code>，该点的几何间隔就是 <code>| w*x + b |/ ||w||</code> ， 即点到超平面的距离。</p>
<p>2）我们开头所说的“要找具有最大间隔的超平面”中的“间隔”指的是几何间隔，也就是最大化（训练集中离超平面最近的点的）点到面距离。</p>
<p>接下来，我们就来构建一个最大间隔分类器来实现学习策略。</p>
<h2 id="最大间隔分类器"><a href="#最大间隔分类器" class="headerlink" title="最大间隔分类器"></a>最大间隔分类器</h2><p>我们假设数据是严格线性可分的，按照上面所说可以得到优化目标：<br><img src="markdown-img-paste-20200622152119434.png" alt=""></p>
<p>约束条件为</p>
<p><img src="markdown-img-paste-20200622152136578.png" alt=""></p>
<p>根据之前的分析，同等程度缩放 w，b 时，函数间隔变化对几何间隔无影响。为了求解方便，这里将函数间隔规定等于 1，即离超平面最近的那些点的函数间隔为 1，则优化目标变成：</p>
<p><img src="markdown-img-paste-20200622152251359.png" alt=""></p>
<p>上述优化目标的几何意义如下图。我们称函数间隔 <code>y*(w*x + b) = 1</code> 的点，即下图中位于平面 <code>w*x + b = 1</code> 或 <code>w*x + b = -1</code> 上的点，为支持向量（support vector）。</p>
<ul>
<li>所有支持向量的函数间隔 <code>y*(w*x + b) = 1</code></li>
<li>所有非支持向量的函数间隔 <code>y*(w*x + b) &gt; 1</code></li>
</ul>
<p><img src="markdown-img-paste-2020062215234943.png" alt=""></p>
<p>接下来，我们将上述优化目标做数学形式的变换</p>
<p><img src="markdown-img-paste-20200622152516616.png" alt=""></p>
<p>这是一个含约束的凸二次规划问题，按书上所说可以直接使用现成的优化计算包（QP 优化包）求解。但是为了更进一步研究 SVM，一般我们将此问题继续变换为其对偶问题并进行求解，原因如下：</p>
<ul>
<li>对偶问题更容易求解</li>
<li>对偶问题的求解中出现了向量内积的形式，从而引入核函数，推广到非线性分类问题。</li>
</ul>
<h3 id="有约束的优化问题-—-拉格朗日乘子法"><a href="#有约束的优化问题-—-拉格朗日乘子法" class="headerlink" title="有约束的优化问题 —- 拉格朗日乘子法"></a>有约束的优化问题 —- 拉格朗日乘子法</h3><p>拉格朗日乘子法和 KKT 条件都是为了求解有约束的优化问题。通过给每个约束条件乘上一个拉格朗日乘子 𝞪，把原来的有约束的优化问题转化为了无约束的优化问题（约束条件隐式地包含在这个无约束的优化问题中）。其中“约束”可分为等式约束和不等式约束。对于含不等式约束的问题，我们需要 KKT 条件（求极值的必要条件）进行求解。即优化问题大致分为以下情况：</p>
<p><strong>1.  无约束优化问题，只需函数对每个变量求导并等于 0（求极值）后求解</strong></p>
<p><strong>2.  只含等式约束的优化问题</strong></p>
<p><img src="markdown-img-paste-20200622152731192.png" alt=""></p>
<p>求解时要构造拉格朗日函数，λ 为拉格朗日乘子</p>
<p><img src="markdown-img-paste-20200622152845823.png" alt=""></p>
<p>通过拉格朗日函数对各个变量 x，λ 求偏导并令其为零（最优解的必要条件），求解</p>
<p><img src="markdown-img-paste-20200622152908761.png" alt=""></p>
<p>得到的解为可能极值点，还需要带入原函数进行验证求最优解。</p>
<p><strong>3.  只含不等式约束的优化问题</strong></p>
<p><img src="markdown-img-paste-20200622153012636.png" alt=""></p>
<p>写出拉格朗日函数</p>
<p><img src="markdown-img-paste-2020062215303654.png" alt=""></p>
<p>通过 KKT 条件（极值的必要条件）求解，</p>
<p><img src="markdown-img-paste-20200622153055518.png" alt="">（ x*  是最优解 ）</p>
<p><strong>4.  同时包含等式和不等式约束的优化问题</strong></p>
<p><img src="markdown-img-paste-20200622153227572.png" alt=""></p>
<p>求解时也要构造拉格朗日函数</p>
<p><img src="markdown-img-paste-20200622153306933.png" alt=""></p>
<p>通过 KKT 条件求解（x 最优解的必要条件）</p>
<p><img src="markdown-img-paste-20200622153256418.png" alt=""></p>
<p>回到 SVM 推导。根据上面的一般理论，SVM 中的优化问题可以写成如下拉格朗日函数</p>
<p><img src="markdown-img-paste-20200622153424253.png" alt=""></p>
<p>上式中，第一部分是优化目标（objective），第二部分是约束条件（constraints）。<br>进一步地，若我们定义<br><img src="markdown-img-paste-20200622153716976.png" alt=""></p>
<p>则原优化问题可以写成拉格朗日函数形式：</p>
<p><img src="markdown-img-paste-20200622153736254.png" alt=""></p>
<p>简单证明一下，为什么上式成立。首先明确，构造的新优化问题是：最小化【拉格朗日函数的最大值】。对于内层问题 —— <strong>最大化拉格朗日函数</strong>：</p>
<ul>
<li>若约束条件满足，即 <code>y*(w*x + b) - 1 ≥ 0</code> 时，由于 <code>𝞪 ≥ 0</code>，拉格朗日函数右边相当于非负数减去非负数，则当函数的第二部分为0时（即只剩原优化目标），函数取最大值。</li>
<li>若任一约束条件不满足时，只要选参数 <code>𝞪 → +∞</code>，拉格朗日函数可以最大化为正无穷，但无法得到优化目标的最小值（无穷大肯定比（1）中结果大，所以这部分计算无意义）。</li>
</ul>
<p>所以，对于内层问题，原约束条件 <code>y*(w*x + b) - 1 ≥ 0</code> 相当于解的可行域，优化只能在可行域上求解（即约束条件“隐藏在”了新优化问题中）。求解原优化问题。该分析过程如下：</p>
<p><img src="markdown-img-paste-20200622153918993.png" alt=""></p>
<p>注意：用拉格朗日乘子法时，相关的约束和求解问题都要与原理论相对应：<br>这里的不等式条件是“≥”，而KKT条件中不等式符号与之相反，所以这里加入一个负号<br>拉格朗日函数一般推导求解的是含约束的 <em>最小化</em> 问题</p>
<h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p>即使有了 KKT 条件去求该优化问题依旧比较复杂，有时转成其偶问题后求解则变得容易很多，先来看看原问题的对偶问题。原优化问题是最小化 𝛉(w)</p>
<p><img src="markdown-img-paste-20200622154052110.png" alt=""></p>
<p>首先，对任意 w，b，要找到一个 𝞪，使得拉格朗日函数都有最大值，最大值即对其它 <code>𝞪’ ≥ 0</code> ，有</p>
<p><img src="markdown-img-paste-20200622154123722.png" alt=""></p>
<p>由于对任意 w，b 上式都成立，为了消除 w，b，（找到合适的 w，b ）使不等式两边取最小值</p>
<p><img src="markdown-img-paste-20200622154133324.png" alt=""></p>
<p>又由于对任意 <code>𝞪’ ≥ 0</code> 上式都成立，为了消除 𝞪’，（可以找到合适的 𝞪’）使不等式右边取最大值</p>
<p><img src="markdown-img-paste-20200622154141279.png" alt=""></p>
<p>至此，我们将原优化问题（极小极大）变成求解其对偶问题（极大极小）。不等式是一个弱对偶问题，取等号时变强对偶问题。数学上，凸二次规划问题 + Slater 条件就可以得到强对偶性，满足强对偶性一定会满足 KKT条件（必要条件），这样我们便可以使用KKT条件来求解原问题的强对偶问题。</p>
<h3 id="求解对偶问题"><a href="#求解对偶问题" class="headerlink" title="求解对偶问题"></a>求解对偶问题</h3><p>上面已经证明了强对偶问题的最优值满足 KKT 条件，接下来利用该条件求解最优值</p>
<p>1）先求 <em>L</em> 对 w，b 的极小，</p>
<p><img src="markdown-img-paste-20200622154244273.png" alt=""></p>
<p>代入 L 得只含 𝞪 的式子</p>
<p><img src="markdown-img-paste-20200622154258718.png" alt=""></p>
<p>具体推导过程如下：<a href="https://blog.csdn.net/v_JULY_v/article/details/7624837" target="_blank" rel="noopener">参考这里</a></p>
<p><img src="markdown-img-paste-20200622154417179.png" alt=""></p>
<p>倒数第三到第二步中 𝞪<sub>i</sub> 和 <em>y</em><sup>(i)</sup> 都是数值，因此转置无影响，倒数第二到最后一步中，使用了乘法运算法则 <code>(a + b + c + … ) (a + b + c + … ) = aa + ab + ac + ba + bb + bc + …</code></p>
<p>2）再求 <em>L</em> 对 𝞪 = { α<sub>1</sub> , α<sub>2</sub> , … , α<sub>n</sub> } 的极大。具体计算通过 <strong>SMO 算法</strong> 完成。</p>
<p><img src="markdown-img-paste-20200622155059313.png" alt=""></p>
<p>3）有了 𝞪 就能计算出 w，b</p>
<p><img src="markdown-img-paste-2020062215513029.png" alt=""></p>
<p>最终得到超平面函数</p>
<p><img src="markdown-img-paste-20200622155139762.png" alt=""></p>
<p>重点：上式表明，给定新数据进行预测，只需要求它与支持向量的内积。这是因为对于非支持向量，由于其对应的拉格朗日乘子 <strong>𝞪<sub>i</sub> 一定为 0</strong>。<br>为什么？我们需要最大化拉格朗日函数，为满足这一要求，第二部分要等于0。</p>
<p><img src="markdown-img-paste-2020062215531661.png" alt=""></p>
<ul>
<li>对于支持向量有 <code>y*(w*x + b) - 1 = 0</code> （因为函数间隔为 1），满足要求。</li>
<li>对于非支持向量有 <code>y*(w*x + b) - 1 &gt; 0</code>，则当且仅当 α<sub>i</sub>  = 0 （α<sub>i</sub>   ≥  0）时，满足要求。</li>
</ul>
<p>因此，第二部分拉格朗日乘子和约束必有一者为 0。这也就说明了，<strong>SVM 中只有离超平面最近的点（这里就是支持向量）有决定力和影响力</strong>。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>上面的内容都是再介绍硬间隔 SVM：对于严格线性可分的数据集，学习策略是硬间隔最大化，用最大间隔分类器来实现，这一问题可通过 “<strong>有约束的优化 →  无约束的优化 →  对偶问题</strong>” 一系列数学转变后进行求解</p>
<p><img src="markdown-img-paste-20200622155853126.png" alt=""></p>
<p>训练好模型（得到超平面）后，预测数据 (x , y) 时，只要代入训练好的函数中，根据结果的正负进行分类。这一点和感知机的预测方式相同。</p>
<h2 id="线性可分-→-线性不可分"><a href="#线性可分-→-线性不可分" class="headerlink" title="线性可分 → 线性不可分"></a>线性可分 → 线性不可分</h2><p>上述过程所求的超平面只能解决线性可分的问题，即硬间隔SVM。当数据集不能使用硬间隔SVM完美分类时，我们视不同情况有两种解决办法：</p>
<ul>
<li>软间隔 SVM：数据集并非是非线性的，误分类点是 outliers，在超平面附近且数量较少时。</li>
<li>升维（低维映射到高维）：数据集是非线性的，线性分类器无法很好分类，产生的误分类点很多。</li>
</ul>
<p>先来看第二种，SVM如何处理非线性数据</p>
<p>一般地，解决线性不可分问题时，常常通过提升维度，将低维原始空间映射到高维特征空间，使数据集在高维空间中线性可分，从而使用线性学习器分类。<strong>如果原始空间为有限维，那么总是存在一个高维特征空间使得样本线性可分</strong>（这一定理支持了非线性支持向量机的可行性）。</p>
<p><strong>举个升维的简单例子：</strong></p>
<p>图中的数据集本身就是非线性的，线性分类不可行，使用一个椭圆分类比较好（如图中的灰色椭圆虚线）。</p>
<p><img src="markdown-img-paste-20200622160100659.png" alt=""></p>
<p>我们设定横坐标为 x<sub>1</sub> ，纵坐标为 x<sub>2</sub> ，这条非线性分隔线的函数为：</p>
<p><img src="markdown-img-paste-20200622160141953.png" alt=""></p>
<p>现在，令 <img src="markdown-img-paste-20200622160332289.png" alt=""> 并构建新坐标系 Z = (z<sub>1</sub> , z<sub>2</sub> , z<sub>3</sub> , z<sub>4</sub> , z<sub>5</sub> ) ，我们会得到一个5维的映射空间，而原来特征空间只有2维 (x<sub>1</sub> , x<sub>2</sub>) 。在这新的5维空间中，分隔函数变为：</p>
<p><img src="markdown-img-paste-20200622160241913.png" alt=""></p>
<p>此时高维特征全是一次项，是一个超平面。在此例中，通过 x 到 z 映射，数据升维，成功地将非线性转为线性问题。</p>
<p><strong>对于硬间隔SVM</strong> ：若 ∅ 代表一个映射，在特征空间中的超平面函数变为：</p>
<p><img src="markdown-img-paste-20200622160309786.png" alt=""></p>
<p>求解方法与之前一样，先写出对应的拉格朗日函数，转成求解对偶问题</p>
<p><img src="markdown-img-paste-20200622160752822.png" alt=""></p>
<p>求出 𝞪，w，b 后可得映射后高维空间的超平面函数</p>
<p><img src="markdown-img-paste-20200622160849906.png" alt=""></p>
<p>看似好像是这么一回事：对于非线性数据，找一个映射，然后把原来的数据映射到新空间中，再做线性 SVM 即可，求解过程只涉及 <strong>高维特征空间中的内积运算</strong> 。问题来了：特征空间的维数可能会非常大，甚至可能出现无穷维，根本无法进行内积运算。</p>
<p>我们先尝试一下上面映射后的内积运算：<br>假设一般二次多项式情况，我们有一个数据集特征分布在d 维空间，<img src="markdown-img-paste-20200622160921418.png" alt=""> 是其特征维度。按照以下映射，将特征升维</p>
<p><img src="markdown-img-paste-20200622160940732.png" alt=""></p>
<p>则高维空间中的内积为</p>
<p><img src="markdown-img-paste-20200622161205332.png" alt=""></p>
<p>计算量是 O (<em>d</em><sup>2</sup>) ，当 <em>d</em> 维度很高，运算量会很可怕。注意：映射内积中的 x<sub><em>i</em></sub> 是向量 x<sub><em>i</em></sub> = ( x<sub>1</sub> , … , x<sub><em>d</em></sub> )，求和符号里的是标量相乘， x 指的是向量 x 中的每一个特征 x<sub><em>i</em></sub> 。</p>
<p>上式进一步转换：</p>
<p><img src="markdown-img-paste-20200622161340954.png" alt=""></p>
<p>等价于</p>
<p><img src="markdown-img-paste-20200622161402249.png" alt=""></p>
<p>这里等式右边的 x<sub><em>i</em></sub> ・x<sub><em>j</em></sub>  是向量内积。上式说明了：映射后高维向量内积可以通过低维向量内积 x<sub><em>i</em></sub> ・x<sub><em>j</em></sub>  计算，计算量变成 O (<em>d</em>) ，大大降低了。</p>
<h2 id="核函数-Kernel"><a href="#核函数-Kernel" class="headerlink" title="核函数 Kernel"></a>核函数 Kernel</h2><p>按照上面分析，我们定义一个核函数</p>
<p><img src="markdown-img-paste-20200622161539510.png" alt=""></p>
<p>其中 𝚽 (x) 则为映射函数。这样，我们在学习和预测中只使用 𝜅 ( x<sub><em>i</em></sub> ・x<sub><em>j</em></sub> )，而非显式计算 𝚽 (x)，但达到的效果一致。这样，核函数成为解决升维所引起的维度灾难的一种方法。</p>
<p>引入核函数后，对偶问题变成</p>
<p><img src="markdown-img-paste-20200622161838872.png" alt=""></p>
<p>超平面函数变成</p>
<p><img src="markdown-img-paste-20200622161933825.png" alt=""></p>
<p>由上式可知核函数直接影响超平面，是影响分类器性能的重要因素之一。选择合适的核函数，才能使数据映射到合适的高维空间中。首先，选择核函数前需要验证核函数的 <strong>有效性</strong>。</p>
<h3 id="核函数的有效性"><a href="#核函数的有效性" class="headerlink" title="核函数的有效性"></a>核函数的有效性</h3><p>给定函数𝜅，能否使用它来替代计算映射的内积 𝚽 (x<sub><em>i</em></sub>)・𝚽 (<sub><em>j</em></sub>)，或是使定义 <img src="markdown-img-paste-20200622162157612.png" alt=""> 成立的映射 𝝓 是否存在？</p>
<blockquote>
<p><strong>Mercer定理</strong>：  设 <img src="markdown-img-paste-20200622162223233.png" alt=""> （从两个 n 维向量映射到实数域），</p>
<p>核函数有效  ⇔  对于任何 <em>X</em> = { x<sup>(<em>1</em>)</sup> , x<sup>(<em>2</em>)</sup> , … , x<sup>(<em>m</em>)</sup> } ，核函数对应的核矩阵必须是对称且半正定的。</p>
</blockquote>
<p>重点：由于核函数是由映射函数内积而成，因此它是一个对称函数，其核矩阵也是对称的。只要核矩阵也是半正定的，那么核函数是有效的，即一定可以找到一个与之对应的映射 𝝓 （使得核函数定义成立）。</p>
<h3 id="核函数的选择"><a href="#核函数的选择" class="headerlink" title="核函数的选择"></a>核函数的选择</h3><p>虽然知道了什么样的核函数有效，但自己构造还是很麻烦的。万幸，有以下几个有效核函数可以使用：</p>
<p><img src="markdown-img-paste-2020062216282268.png" alt=""></p>
<p>在 sklearn 中核函数的定义（超参数有所改变）【截图： sklearn 官方文档】</p>
<p><img src="markdown-img-paste-20200622162848235.png" alt=""></p>
<p>其中，<strong>高斯核（Radial Basis Function Kernel， RBF）</strong>，又称 <strong>径向基函数</strong>，对应 <strong>无限维度</strong> 特征映射函数。</p>
<p>sklearn 中的 γ 其实就是高斯函数中的 ½ * σ<sup>2</sup>  。根据推导，带高斯核的 SVM 的分类函数，相当于 n 个高斯函数的线性组合，每个高斯函数的中心都是 SV，所以每个 SV 的高斯分布都影响最终的分类面。下图是 γ 变化（1，10，100）对应的分类效果【截图：台大林轩田《机器学习技法》课程】</p>
<p><img src="markdown-img-paste-20200622163347397.png" alt=""></p>
<ul>
<li>γ 越小，σ<sup>2</sup> 越大，即分布越宽时，每个分布容易重叠融合在一起，模型越简单；</li>
<li>γ 越大，σ<sup>2</sup> 越小，即分布越窄时，容易出现独立边界，模型越复杂。</li>
</ul>
<h3 id="对比三种核"><a href="#对比三种核" class="headerlink" title="对比三种核"></a>对比三种核</h3><p><strong>线性核（Linear Kernel）</strong><br>简单快速<br>当数据线性不可分时就不能使用了</p>
<p><strong>多项式核（Polynomial Kernel）</strong><br>优于线性核，可以处理非线性数据<br>当阶数 d 过高时，κ ( . ) 数值范围很大<br>超参数有三个，不容易确定</p>
<p><strong>高斯核（Radial Basis Function Kernel）</strong><br>模型更加复杂多样，并且超参数只有一个<br>模型过于复杂时，易过拟合。</p>
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h4><p><strong>为什么 SVM 要用核函数？</strong></p>
<ul>
<li>遇到非线性数据集，一般地，使用升维方法将数据映射到高维空间中，使其变得线性可分。</li>
<li>但是，当映射后的空间维度很高甚至无限时，会引发维度灾难。在SVM中，求解超平面时需要用到（数据）向量内积，若直接在高维空间求解高维向量的内积，会非常复杂。</li>
<li>使用核函数可以避免直接在高维空间中进行计算。它虽然也是将特征升维，但先在低维上计算，而分类效果表现在了高维上（低维计算，高维表现）。</li>
</ul>
<p><strong>SVM 如何使用核函数？</strong></p>
<ul>
<li>SVM中先定义核函数表示的是向量映射后的内积（因为SVM分类函数要用到内积，所以这么定义方便计算）。</li>
<li>有了定义，当我们选择某个具体形式的核函数（高斯核、多项式核），只需要将输入空间的样本带进具体核函数进行运算，得到的结果我们认为就是映射后的高维向量内积结果（ <em>mercer定理支撑：当你选择的是对称函数且构成的矩阵是半正定的，则其为有效核函数 ⇒ 一定存在映射使核函数的定义成立，即核函数计算结果就是映射后内积</em> ）</li>
</ul>
<p>【注】kernel，内积，相似度可以说是同一种东西，kernel 定义为映射的内积，内积可以衡量两个向量相似度。</p>
<h2 id="软间隔-SVM"><a href="#软间隔-SVM" class="headerlink" title="软间隔 SVM"></a>软间隔 SVM</h2><p>回到之前的问题，当硬间隔 SVM 不能完美地将数据集分离，当数据集本身是非线性时，我们可以采用升维和核函数来解决。而对于由 outliers（少量异常点，如噪声数据）引起的线性不可分，用软间隔 SVM 解决。</p>
<p>软间隔 SVM 学习策略：</p>
<ul>
<li>允许某些数据点不满足约束 <code>y*(w*x + b) ≥ 1</code></li>
<li>不满足约束的数据要尽可能少</li>
</ul>
<p>对于不满足约束的点所引起的损失也要尽可能的小，因此，原来的优化问题变成</p>
<p><img src="markdown-img-paste-20200622164414184.png" alt=""></p>
<p>这里的损失函数一般会选择与距离相关的（可导） hinge 损失（若选择 0/1 损失是阶跃函数，不可导）</p>
<p><img src="markdown-img-paste-20200622164654971.png" alt=""></p>
<p>其函数图像为（横坐标为 z，纵坐标为 loss 值）</p>
<p><img src="markdown-img-paste-20200622164721539.png" alt=""></p>
<p>此时优化问题变成（ C 是一个超参数，控制优化目标中的两项的权重）</p>
<p><img src="markdown-img-paste-20200622164837422.png" alt=""></p>
<ul>
<li>若 z ≥ 1，数据点正确分类，<code>loss = 0</code>；</li>
<li>若 z &lt; 1，数据点误分类，<code>loss = 1 - z</code></li>
</ul>
<p>令 <code>z  =  yi * (w*xi + b)</code>，则有</p>
<p><img src="markdown-img-paste-20200622164851848.png" alt=""></p>
<ul>
<li>若  <code>yi * (w*xi + b) ≥ 1</code>，即约束条件满足，<code>loss = 0</code>；</li>
<li>若  <code>yi * (w*xi + b) &lt; 1</code>，即约束条件不满足时，<code>loss = 1- yi * (w*xi + b)</code></li>
</ul>
<p>为了简化上式，因为 loss 是个非负数，再令 <em>ξ</em><sub><em>i</em></sub>  =  1 - z   =  1 - y<sub><em>i</em></sub> * (w * x<sub><em>i</em></sub> + b)，软间隔 SVM 的目标函数为</p>
<p><img src="markdown-img-paste-20200622165106474.png" alt="">  （ <em>ξ</em><sub><em>i</em></sub>  称作松弛变量 ）</p>
<p>同硬间隔 SVM 求解一样，带约束的优化问题需要构建拉格朗日函数（两种约束条件则有两乘子 <em>α</em> ，<em>r</em>，函数变量有 w，<em>b</em>，<em>ξ</em> ）</p>
<p><img src="markdown-img-paste-20200622165210168.png" alt=""></p>
<p>同样转换成 <strong>对偶问题</strong> 后，先求函数对各个变量的偏导令为 0，</p>
<p><img src="markdown-img-paste-20200622165241944.png" alt=""></p>
<p>同样地，将 w 代回函数，使函数只含 <em>α</em>，我们发现和硬间隔 SVM 目标函数一样，但约束条件有变化（ <strong><em>α</em> 多了上限</strong>）</p>
<p><img src="markdown-img-paste-2020062216533710.png" alt=""></p>
<p><em>α</em> 的求解一样也可以通过 <strong>SMO算法</strong> ，之后就可以得到软间隔 SVM 的超平面了。另外，软间隔也可以用 Kernel 函数处理非线性数据，只要把 <strong>x<sub><em>i</em></sub> ・x<sub><em>j</em></sub></strong>  换成  <strong>𝜅 ( x<sub><em>i</em></sub> ・x<sub><em>j</em></sub> )</strong> 即可。</p>
<h2 id="SVM-分类器优缺点"><a href="#SVM-分类器优缺点" class="headerlink" title="SVM 分类器优缺点"></a>SVM 分类器优缺点</h2><h4 id="SVM-的优点"><a href="#SVM-的优点" class="headerlink" title="SVM 的优点"></a>SVM 的优点</h4><ol>
<li><p>解决小样本下机器学习问题；</p>
</li>
<li><p>解决非线性问题；</p>
</li>
<li><p>无局部极小值问题（相对于神经网络等算法）；</p>
</li>
<li><p>可以很好的处理高维数据集；模型只受边界线附近的点的影响，因此对于高维数据的学习效果非常好  —— 即使维度比样本数量还要高的数据也没有问题，这是其他算法难以企及的；</p>
</li>
<li><p>该算法是结构经验最小化算法，自带正则化项（ ½ * ||w||<sup>2</sup> ）。比较 SVM 算法和 L2 正则化可以发现，L2 正则项和 SVM 的优化目标是一样的，soft-margin SVM 的损失项和含 L2 正则化问题的优化目标 <em>E</em><sub><em>in</em></sub> 也是一样的。这么看来优化目标和约束条件在 SVM 和带 L2 正则化的问题中分别对调了。  </p>
</li>
</ol>
<p><img src="markdown-img-paste-20200622170015932.png" alt=""><br><img src="markdown-img-paste-20200622170026716.png" alt=""></p>
<ol start="6">
<li><p>泛化能力比较强；</p>
</li>
<li><p>模型依赖的支持向量比较少，消耗的内存少；</p>
</li>
<li><p>一旦模型训练完成，预测阶段的速度非常快；</p>
</li>
<li><p>核技巧能够适用于不同类型的数据。</p>
</li>
</ol>
<blockquote>
<p>1）经验风险最小化：求解问题的最优解</p>
<p>2）结构风险最小化：正则化（在经验风险最小化基础上加上表示模型复杂度的正则化项）</p>
</blockquote>
<h4 id="SVM-的缺点"><a href="#SVM-的缺点" class="headerlink" title="SVM 的缺点"></a>SVM 的缺点</h4><ol>
<li><p>对于核函数的高维映射解释力不强，尤其是径向基函数；</p>
</li>
<li><p>对缺失数据敏感；</p>
</li>
<li><p>随着样本数量的增加，时间复杂度提升，计算成本高；</p>
</li>
<li><p>训练效果依赖于超参数 C 的选择，需要交叉验证来搜索，数据集大的时候，计算量也很大；</p>
</li>
<li><p>无法进行概率计算，但 scikit-learn 支持计算概率。</p>
</li>
</ol>
<h4 id="对比感知机："><a href="#对比感知机：" class="headerlink" title="对比感知机："></a>对比感知机：</h4><p>感知机数据必须是严格线性可分的，否则（在不限制迭代次数时）算法无法收敛。<br>感知机的决策边界是不唯一的，而 SVM 得到的决策边界是唯一的。对于未知的数据， SVM 有更好的预测能力。<br>二者训练模型的方法不同，但是训练结果都是超平面（相关参数w，b）。对新数据的预测方式一样：输入（x，y），当分类函数值大于0，为 +1 类， 反之为 -1 类。</p>
<h4 id="对比-Logistic-回归："><a href="#对比-Logistic-回归：" class="headerlink" title="对比 Logistic 回归："></a>对比 Logistic 回归：</h4><p>将逻辑回归中使用的 g(z) 函数从 sigmoid 变成</p>
<p><img src="markdown-img-paste-20200622170436384.png" alt=""></p>
<p>即标签从 {0，1} 变到了 {-1，1} 。并且，假设函数用 w，b 表示： h<sub>𝜽</sub> (x) = g (𝜽<sup>T</sup>x)  →   h<sub>w,b</sub>(x) = g (b + w<sup>T</sup>x)</p>
<ul>
<li>SVM考虑 <strong>局部最大化</strong>，强调让靠近中间线的点尽可能的远离中间线;</li>
<li>Logistic考虑 <strong>全局最大化</strong>，强调所有点都尽可能的远离超平面，使所有点都被尽可能的分类。</li>
</ul>
<p>两种方法都是常见的分类算法</p>
<ol>
<li><p>从目标函数来看，逻辑回归采用的是logistical loss，svm采用的是hinge loss。这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。SVM只考虑support vectors（和分类最相关的少数点）。逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。两者的根本目的都是一样的。</p>
</li>
<li><p>两个方法都可以增加不同的正则化项。所以在很多实验中，两种算法的结果是很接近的。逻辑回归模型更简单，特别是大规模线性分类时比较方便。SVM的理解和优化相对来说复杂一些，但 SVM的原问题转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，当计算核函数时优势很明显。</p>
</li>
<li><p>两者对异常的敏感度也不一样。同样的线性分类情况下，如果异常点较多无法剔除时，LR中每个样本都是有贡献的，最大似然后会自动压制异常的贡献，SVM+软间隔对异常还是比较敏感，因为其训练只需要支持向量，有效样本较少，一旦被干扰，预测结果难以预料。</p>
</li>
</ol>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Cecilia Song
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://yoursite.com/2020/06/22/post/SVM/" title="支持向量机（Support Vector Machine）">http://yoursite.com/2020/06/22/post/SVM/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/SVM/" rel="tag"># SVM</a>
          
            <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/06/20/post/TCP-IP-UDP/" rel="next" title="TCP/IP UDP 总结">
                <i class="fa fa-chevron-left"></i> TCP/IP UDP 总结
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
	    <a href='/'>
              <img class="site-author-image" itemprop="image"
                src="/images/selficon.png"
                alt="Cecilia Song" />
            </a>
	    
              <p class="site-author-name" itemprop="name">Cecilia Song</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
		<a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Cecilia9999" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:yxsong924@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#两个间隔定义"><span class="nav-number">1.</span> <span class="nav-text">两个间隔定义</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-函数间隔（functional-margin）"><span class="nav-number">1.1.</span> <span class="nav-text">1.   函数间隔（functional margin）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-几何间隔（geometrical-margin）"><span class="nav-number">1.2.</span> <span class="nav-text">2.   几何间隔（geometrical margin）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么最大化“间隔”用的是几何而非函数间隔："><span class="nav-number">1.3.</span> <span class="nav-text">为什么最大化“间隔”用的是几何而非函数间隔：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#总结："><span class="nav-number">1.3.1.</span> <span class="nav-text">总结：</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最大间隔分类器"><span class="nav-number">2.</span> <span class="nav-text">最大间隔分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#有约束的优化问题-—-拉格朗日乘子法"><span class="nav-number">2.1.</span> <span class="nav-text">有约束的优化问题 —- 拉格朗日乘子法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#对偶问题"><span class="nav-number">3.</span> <span class="nav-text">对偶问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#求解对偶问题"><span class="nav-number">3.1.</span> <span class="nav-text">求解对偶问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#总结"><span class="nav-number">3.1.1.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性可分-→-线性不可分"><span class="nav-number">4.</span> <span class="nav-text">线性可分 → 线性不可分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#核函数-Kernel"><span class="nav-number">5.</span> <span class="nav-text">核函数 Kernel</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#核函数的有效性"><span class="nav-number">5.1.</span> <span class="nav-text">核函数的有效性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#核函数的选择"><span class="nav-number">5.2.</span> <span class="nav-text">核函数的选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对比三种核"><span class="nav-number">5.3.</span> <span class="nav-text">对比三种核</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#总结-1"><span class="nav-number">5.3.1.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#软间隔-SVM"><span class="nav-number">6.</span> <span class="nav-text">软间隔 SVM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SVM-分类器优缺点"><span class="nav-number">7.</span> <span class="nav-text">SVM 分类器优缺点</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM-的优点"><span class="nav-number">7.0.1.</span> <span class="nav-text">SVM 的优点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM-的缺点"><span class="nav-number">7.0.2.</span> <span class="nav-text">SVM 的缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对比感知机："><span class="nav-number">7.0.3.</span> <span class="nav-text">对比感知机：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#对比-Logistic-回归："><span class="nav-number">7.0.4.</span> <span class="nav-text">对比 Logistic 回归：</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
      
      
        <script type="text/javascript" charset="utf-8" src="/js/tagcloud.js"></script>
  	<script type="text/javascript" charset="utf-8" src="/js/tagcanvas.js"></script>
  	<div class="widget-wrap">
    	  <h3 class="widget-title">Tag Cloud</h3>
    	  <div id="myCanvasContainer" class="widget tagcloud">
            <canvas width="250" height="250" id="resCanvas" style="width:100%">
              <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/" rel="tag">SVM</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TCP-IP/" rel="tag">TCP/IP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UDP/" rel="tag">UDP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/git/" rel="tag">git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/github/" rel="tag">github</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/" rel="tag">hexo</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/" rel="tag">machine learning</a><span class="tag-list-count">1</span></li></ul>
      	    </canvas>
    	  </div>
        </div>
      		
    
    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Cecilia Song</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
